# Copyright 2022 – present by the UBC EOAS MOAD Group and The University of British Columbia.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

"""Extract diatom and lon/lat fields from day-averaged SalishSeaCast hindcast files
to create long time series files that can be processed to generate “nudging” fields for Atlantis.

This is a module implementation of code developed in
https://nbviewer.org/github/SalishSeaCast/analysis-doug/blob/main/notebooks/dask-expts/atlantis_nudge_diatoms.ipynb
It has a minimal command-line interface:

  python3 -m atlantis_nudge_diatoms start_date end_date n_workers n_threads_per_worker nc_dest_dir

e.g.

  python3 -m atlantis_nudge_diatoms 2007-01-01 2007-01-31 4 10 /ocean/dlatorne/Atlantis/day-avg-diatoms/

This is a stepping stone in the design of the Reshapr package.
"""


import functools
import time
from pathlib import Path
import sys

import arrow
import dask.distributed
import numpy
import xarray


RESULTS_ARCHIVE = Path("/results2/SalishSea/nowcast-green.201905/")
SSC_CHUNKS = {"time_counter": 1, "deptht": 40, "y": 898, "x": 398}
GRID_URL = "https://salishsea.eos.ubc.ca/erddap/griddap/ubcSSnBathymetryV17-02"


def main(start_date, end_date, nc_dest_dir):
    date_range = arrow.Arrow.range("days", start_date, end_date)
    ds_paths = [
        RESULTS_ARCHIVE
        / ddmmmyy(day)
        / f"SalishSea_1d_{yyyymmdd(day)}_{yyyymmdd(day)}_ptrc_T.nc"
        for day in date_range
    ]
    with open_reduced_dataset(ds_paths, SSC_CHUNKS, {"diatoms"}) as ds:

        times, depths, y_indices, x_indices = calc_coords(ds)
        diatoms, lons, lats = calc_variables(
            ds, GRID_URL, times, depths, y_indices, x_indices
        )

        extracted_ds_name = (
            f"SalishSeaCast_day_avg_diatoms_{yyyymmdd(start_date)}_{yyyymmdd(end_date)}"
        )
        extracted_ds = xarray.Dataset(
            coords={
                "time": times,
                "depth": depths,
                "gridY": y_indices,
                "gridX": x_indices,
            },
            data_vars={
                "longitude": lons,
                "latitude": lats,
                "diatoms": diatoms,
            },
            attrs={
                "name": extracted_ds_name,
                "description": "Day-averaged diatoms biomass extracted from SalishSeaCast v201905 hindcast",
                "history": (
                    f"{arrow.now('PST').format('YYYY-MM-DD HH:mm')}: "
                    f"Generated by analysis-doug/notebooks/dask-expts/atlantis_nudge_diatoms.py"
                ),
            },
        )

        write_netcdf(extracted_ds, nc_dest_dir)


def ddmmmyy(arrow_date):
    """Return an Arrow date as a string formatted as lower-cased `ddmmmyy`."""
    return arrow_date.format("DDMMMYY").lower()


def yyyymmdd(arrow_date):
    """Return an Arrow date as a string of digits formatted as `yyyymmdd`."""
    return arrow_date.format("YYYYMMDD")


def function_timer(func):
    @functools.wraps(func)
    def wrapper_function_timer(*args, **kwargs):
        t_start = time.time()
        return_value = func(*args, **kwargs)
        t_end = time.time()
        print(f"{func.__name__}(): {t_end - t_start}s")
        return return_value

    return wrapper_function_timer


@function_timer
def open_reduced_dataset(ds_paths, chunks, keep_vars):
    # Use 1st dataset path to calculate the set of variables to drop
    with xarray.open_dataset(ds_paths[0], chunks=chunks) as ds:
        drop_vars = {var for var in ds.data_vars} - keep_vars
    # Return dataset with only variables of interest for full list of paths
    # This triggers a dask task graph computation
    return xarray.open_mfdataset(ds_paths, chunks=chunks, drop_variables=drop_vars)


def calc_coords(ds):
    times = create_dataarray(
        "time",
        ds.time_counter,
        attrs={
            "standard_name": "time",
            "long_name": "Time Axis",
            "comment": (
                "time values are UTC at the centre of the intervals over which the calculated model results are averaged;"
                "e.g. the field average values for 01 January 2007 have a time value of 2007-01-01 12:00:00Z"
            ),
            # time_origin and units are provided by encoding when dataset is written to netCDF file
        },
    )
    depths = create_dataarray(
        "depth",
        ds.deptht,
        attrs={
            "standard_name": "sea_floor_depth",
            "long_name": "Sea Floor Depth",
            "units": "metres",
        },
    )
    y_indices = create_dataarray(
        "gridY",
        ds.y,
        attrs={
            "standard_name": "y",
            "long_name": "Grid Y",
            "units": "count",
            "comment": "gridY values are grid indices in the model y-direction",
        },
    )
    x_indices = create_dataarray(
        "gridX",
        ds.x,
        attrs={
            "standard_name": "x",
            "long_name": "Grid X",
            "units": "count",
            "comment": "gridX values are grid indices in the model x-direction",
        },
    )
    return times, depths, y_indices, x_indices


def calc_variables(ds, grid_url, times, depths, y_indices, x_indices):
    diatoms = create_dataarray(
        "diatoms",
        ds.diatoms,
        attrs={
            "standard_name": "mole_concentration_of_diatoms_expressed_as_nitrogen_in_sea_water",
            "long_name": "Diatoms Concentration",
            "units": "mmol m-3",
        },
        coords={
            "time": times,
            "depth": depths,
            "gridY": y_indices,
            "gridX": x_indices,
        },
    )
    with xarray.open_dataset(grid_url) as grid_ds:
        lons = create_dataarray(
            "longitude",
            grid_ds.longitude,
            attrs={
                "standard_name": "longitude",
                "long_name": "Longitude",
                "units": "degrees_east",
            },
            coords={
                "gridY": y_indices,
                "gridX": x_indices,
            },
        )
        lats = create_dataarray(
            "latitude",
            grid_ds.latitude,
            attrs={
                "standard_name": "latitude",
                "long_name": "Latitude",
                "units": "degrees_north",
            },
            coords={
                "gridY": y_indices,
                "gridX": x_indices,
            },
        )
    return diatoms, lons, lats


def create_dataarray(name, source_data_var, attrs, coords: str | dict = "data_var"):
    return xarray.DataArray(
        name=name,
        data=source_data_var.data,
        coords={name: source_data_var.data} if coords == "data_var" else coords,
        attrs=attrs,
    )


@function_timer
def write_netcdf(extracted_ds, nc_dest_dir):
    encoding = {}
    for coord in extracted_ds.coords:
        encoding[coord] = calc_coord_encoding(extracted_ds, coord)
    for v_name, v_array in extracted_ds.data_vars.items():
        encoding[v_name] = calc_var_encoding(
            v_array, ("time", "depth", "gridY", "gridX")
        )
    nc_path = nc_dest_dir / f"{extracted_ds.name}.nc"
    extracted_ds.to_netcdf(
        nc_path, format="NETCDF4_CLASSIC", encoding=encoding, unlimited_dims="time"
    )


def calc_coord_encoding(ds, coord):
    match coord:
        case "time":
            return {
                "dtype": numpy.single,
                "units": "days since 2007-01-01 12:00:00",
                "chunksizes": [1],
            }
        case "depth":
            return {"dtype": numpy.single, "chunksizes": [ds.coords[coord].size]}
        case _:
            return {"dtype": int, "chunksizes": [ds.coords[coord].size]}


def calc_var_encoding(var, coord_names):
    chunksizes = []
    for c_name in coord_names:
        try:
            chunksizes.append(var.coords[c_name].size)
        except KeyError:
            pass
    if "time" in var.coords:
        chunksizes[0] = 1
    return {
        "dtype": numpy.single,
        "chunksizes": chunksizes,
    }


if __name__ == "__main__":
    t_start = time.time()
    # Command-line interface:
    #   python3 -m atlantis_nudge_diatoms start_date end_date n_workers n_threads_per_worker nc_dest_dir
    args = sys.argv[1:]
    start_date = arrow.get(args[0])
    end_date = arrow.get(args[1])
    n_workers = int(args[2])
    n_threads = int(args[3])
    nc_dest_dir = Path(args[4])
    client = dask.distributed.Client(
        n_workers=n_workers,
        threads_per_worker=n_threads,
        processes=True,
    )
    main(start_date, end_date, nc_dest_dir)
    client.close()
    t_end = time.time()
    print(f"total: {t_end - t_start}s")
